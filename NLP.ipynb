{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SriDharshini16/Python/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDmwvYqh-2rt"
      },
      "source": [
        "TOKENISATION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EgS3Ywl7vPM"
      },
      "outputs": [],
      "source": [
        "text=\"My name is Sri Dharshini\" #splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woiTR_XN9qeB",
        "outputId": "b969b1eb-b0b5-4cbc-de84-ec55ec593022"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['My', 'name', 'is', 'Sri', 'Dharshini']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AK2Hj_lQ97Vm"
      },
      "outputs": [],
      "source": [
        "text=\"Who are you? My name is Sri Dharshini. I am studying at BIT.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jfv5cWDR-cKn",
        "outputId": "d953ec9f-50e3-4dc8-ae4f-d86931873bbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Who are you? My name is Sri Dharshini', ' I am studying at BIT', '']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text.split('.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeMQC4xW_xpb"
      },
      "source": [
        "**USING REGEX**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC7gMEqe-kgy",
        "outputId": "c58deeed-63a5-490e-dd8c-289854f69d83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Who are you', ' My name is Sri Dharshini', ' I am studying at BIT', '']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "text=\"Who are you? My name is Sri Dharshini. I am studying at BIT.\"\n",
        "text=re.compile('[.?]').split(text)\n",
        "text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME6PiBiUAjN-"
      },
      "source": [
        "**NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfvQcULk_f7y"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBMXXwThA7-X",
        "outputId": "a89a32b2-5e54-477e-8604-9fcb123029d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBkaxArOBGVi",
        "outputId": "fd05737c-8ec1-4b57-a8b9-8befd09e6109"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Who',\n",
              " 'are',\n",
              " 'you',\n",
              " '?',\n",
              " 'My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Sri',\n",
              " 'Dharshini',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'studying',\n",
              " 'at',\n",
              " 'BIT',\n",
              " '.']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence=\"Who are you? My name is Sri Dharshini. I am studying at BIT.\"\n",
        "word_tokenize(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KyHhcEtBuJc"
      },
      "source": [
        "**TOKENIZATION SENTENCE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPjQvuJxBUfM",
        "outputId": "7257ea7b-d029-43b6-85de-3b0f9c6c6207"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Who are you?', 'My name is Sri Dharshini.', 'I am studying at BIT.']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y7R9aJBCTXP"
      },
      "source": [
        "**STEMMING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBIOcO66CEXD"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer #to get the root word\n",
        "porter=PorterStemmer()\n",
        "words=['say','saying','said','says']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrIW542XC7LU",
        "outputId": "68a20d75-221b-4007-d7b5-3c543be8e9be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "say --> say\n",
            "saying --> say\n",
            "said --> said\n",
            "says --> say\n"
          ]
        }
      ],
      "source": [
        "for word in words:\n",
        "  print(word,\"-->\",porter.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnJo5ZS_DgcX",
        "outputId": "3c2b5988-285c-4a14-b1b9-290ae1578ac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "say --> say\n",
            "saying --> say\n",
            "said --> said\n",
            "says --> say\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import SnowballStemmer \n",
        "snowball=SnowballStemmer(language=\"english\")\n",
        "for word in words:\n",
        "  print(word,\"-->\",snowball.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88VajFkEFtHv",
        "outputId": "ac413f6c-7483-4f69-eb2d-64764bb4ac4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "talk --> talk\n",
            "talking --> talk\n",
            "told --> told\n"
          ]
        }
      ],
      "source": [
        "dict=['talk','talking','told']\n",
        "from nltk.stem import LancasterStemmer\n",
        "lancaster=SnowballStemmer(language=\"english\")\n",
        "\n",
        "for word in dict:\n",
        "  print(word,\"-->\",lancaster.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "62DaVLYjJosf",
        "outputId": "3cc6ffb8-6c70-4ea3-a80f-f7775d1c7a91"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e49d9c9e75d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'talk'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'talking'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'told'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mregexp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRegexpStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'regexp'"
          ]
        }
      ],
      "source": [
        "word=['talk','talking','told']\n",
        "from nltk.stem import RegexpStemmer\n",
        "regexp=RegexpStemmer()\n",
        "\n",
        "for word in words:\n",
        "  print(word,\"-->\",regexp.stem(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AyhZIY-LZMJ"
      },
      "source": [
        "**LEMMATIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnwFBwwSKPxt",
        "outputId": "06e68a8b-1e62-483d-e7ee-f3ec2f1764c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#converting to root word\n",
        "from textblob import TextBlob,Word\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLq7IgaKMSBy",
        "outputId": "39186dad-eca8-4aeb-f0ab-631180b16e1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "said--> said\n",
            "wait--> wait\n",
            "carrying--> carrying\n",
            "punches--> punch\n"
          ]
        }
      ],
      "source": [
        "list=['said','wait','carrying','punches']\n",
        "for word in list:\n",
        "  w=Word(word)\n",
        "  print(word+'-->',w.lemmatize())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKPkY_YwNSI-",
        "outputId": "c4465cc9-8969-479f-d990-e4d1d7e06c6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "said --> said\n",
            "wait --> wait\n",
            "carrying --> carrying\n",
            "punches --> punch\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wn1=WordNetLemmatizer()\n",
        "for word in list:\n",
        "  print(word,'-->',wn1.lemmatize(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZH86b2NPbgG"
      },
      "source": [
        "**BOW(BAG OF WORDS)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4AdC-yqYQxn"
      },
      "source": [
        "Representing words to numbers(If the words in vocabulary are presented in sentence it is represented by 1 or else 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxrtGggOYLYj"
      },
      "source": [
        "**COSINE SIMILARITY(VECTOR REPRESENTATION)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM4PyNXKV6Sg"
      },
      "source": [
        "TF(Term Frequency)=no of times that term occured/total no of words\n",
        "\n",
        "IDF=log(total no of quieres/in how many quieries it occured)\n",
        "\n",
        "TF*IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpEg-LzJaHhJ"
      },
      "outputs": [],
      "source": [
        "import en_core_web_sm\n",
        "nlp= en_core_web_sm.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfzC3zWJa7xr",
        "outputId": "0cc2df6d-0c62-4076-ec64-f2a3d756a407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7598359852037434\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        }
      ],
      "source": [
        "sen1= nlp(\"HOW CAN I CHECK MY BALANCE?\")\n",
        "sen2= nlp(\"I WANT TO CHECK MY BALANCE.\")\n",
        "print(sen2.similarity(sen1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm4MW8dUbuZ6",
        "outputId": "6c78c315-ae20-492f-a6a9-975a150fadad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ikq8Y0Uwb1A6",
        "outputId": "cd53549e-504c-42f9-cfa1-54c86e0cdce1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCUWcz5xcNy1",
        "outputId": "7b9019a9-fd70-4fbf-cbe8-099338a2fbd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fmfiAtHcfd_",
        "outputId": "fa90e375-7d8c-4de6-b7df-39a4b8521562"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['How', 'can', 'I', 'check', 'my', 'account', 'balance']"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stop_words=set(stopwords.words('english'))\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokens_1=word_tokenize(\"How can I check my account balance\")\n",
        "word_tokens_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwGKY6-6dQEH",
        "outputId": "3ffc69f6-5e50-43b3-f48d-d032ca1883f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "check account balance\n"
          ]
        }
      ],
      "source": [
        "new_sen_1=[w for w in word_tokens_1 if w.lower() not in stop_words]\n",
        "new_sen_1=' '.join(str(elem) for elem in new_sen_1)\n",
        "print(new_sen_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hikUgTD2eTDq",
        "outputId": "bea1c292-e476-40d5-dcaf-6a7a762dfd3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "want check account balance\n"
          ]
        }
      ],
      "source": [
        "word_tokens_2=word_tokenize(\"I want to check my account balance\")\n",
        "new_sen_2=[w for w in word_tokens_2 if w.lower() not in stop_words]\n",
        "new_sen_2=' '.join(str(elem) for elem in new_sen_2)\n",
        "print(new_sen_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te7617QJfiAV",
        "outputId": "08302655-b3f9-4231-d7c4-c4b8e4833132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9003613092570767\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        }
      ],
      "source": [
        "new_sen_1=nlp(new_sen_1)\n",
        "new_sen_2=nlp(new_sen_2)\n",
        "print(new_sen_2.similarity(new_sen_1))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "NLP Workshop",
      "provenance": [],
      "authorship_tag": "ABX9TyO6WBsVVr1/STbFE730M+65",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}